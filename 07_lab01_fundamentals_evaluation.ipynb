{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 7 - Lab 01: ML Fundamentals & Evaluation Practice\n",
    "**Instructor:** Amir Charkhi | **Type:** Hands-On Practice\n",
    "\n",
    "> Practice what you learned in Notebooks 01 & 02\n",
    "\n",
    "## üéØ Lab Objectives\n",
    "\n",
    "In this lab, you'll practice:\n",
    "- Loading data and performing train/test split\n",
    "- Training your first models\n",
    "- Calculating and interpreting evaluation metrics\n",
    "- Understanding confusion matrices\n",
    "- Choosing the right metric for the problem\n",
    "\n",
    "**Time**: 30-40 minutes  \n",
    "**Difficulty**: ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ (Beginner)\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Quick Reference\n",
    "\n",
    "**Classification Metrics:**\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "```\n",
    "\n",
    "**Regression Metrics:**\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete! Let's start practicing!\n"
     ]
    }
   ],
   "source": [
    "# Setup - Run this cell first!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Setup complete! Let's start practicing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Exercise 1: Train/Test Split Basics\n",
    "\n",
    "Let's practice the most fundamental concept in ML!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded!\n",
      "Number of samples: 569\n",
      "Number of features: 30\n",
      "Classes: [0 1]\n",
      "class distribution: 1    357\n",
      "0    212\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "print(f\"Dataset loaded!\")\n",
    "print(f\"Number of samples: {len(X)}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Classes: {np.unique(y)}\")\n",
    "\n",
    "# TODO 1.1: Print the class distribution (how many 0s and 1s)\n",
    "# Hint: Use np.bincount(y) or pd.Series(y).value_counts()\n",
    "\n",
    "# Your code here:\n",
    "print(f\"class distribution: {pd.Series(y).value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Perform Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training set: 455 samples\n",
      "‚úÖ Test set: 114 samples\n",
      "‚úÖ Train/test split: 80% / 20%\n",
      "\n",
      "üéâ Task 1.2 Complete!\n"
     ]
    }
   ],
   "source": [
    "# TODO 1.2: Split the data into train and test sets\n",
    "# Requirements:\n",
    "#   - Use 80% for training, 20% for testing\n",
    "#   - Set random_state=42 for reproducibility\n",
    "#   - Use stratify=y to maintain class balance\n",
    "\n",
    "# Your code here:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=42, stratify=y) \n",
    "\n",
    "# Validation (Don't modify)\n",
    "print(f\"‚úÖ Training set: {len(X_train)} samples\")\n",
    "print(f\"‚úÖ Test set: {len(X_test)} samples\")\n",
    "print(f\"‚úÖ Train/test split: {len(X_train)/len(X)*100:.0f}% / {len(X_test)/len(X)*100:.0f}%\")\n",
    "\n",
    "assert len(X_train) + len(X_test) == len(X), \"‚ùå Split doesn't add up!\"\n",
    "assert len(X_train) > len(X_test), \"‚ùå Training set should be larger!\"\n",
    "print(\"\\nüéâ Task 1.2 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Verify Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    285\n",
       "0    170\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    72\n",
       "0    42\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 percentage:\n",
      "  Original: 62.7%\n",
      "  Training: 62.6%\n",
      "  Test:     63.2%\n",
      "\n",
      "‚úÖ Stratification worked! Proportions are similar.\n",
      "üéâ Task 1.3 Complete!\n"
     ]
    }
   ],
   "source": [
    "###### TODO 1.3: Calculate and compare class proportions\n",
    "# Calculate the percentage of class 1 (positive class) in:\n",
    "#   - Original dataset (y)\n",
    "#   - Training set (y_train)\n",
    "#   - Test set (y_test)\n",
    "\n",
    "# Your code here:\n",
    "original_class1_pct = (357/len(y))\n",
    "train_class1_pct = (285/len(y_train))\n",
    "test_class1_pct = (72/len(y_test))\n",
    "\n",
    "# Validation (Don't modify)\n",
    "print(f\"Class 1 percentage:\")\n",
    "print(f\"  Original: {original_class1_pct:.1%}\")\n",
    "print(f\"  Training: {train_class1_pct:.1%}\")\n",
    "print(f\"  Test:     {test_class1_pct:.1%}\")\n",
    "\n",
    "if abs(train_class1_pct - original_class1_pct) < 0.02:  # Within 2%\n",
    "    print(\"\\n‚úÖ Stratification worked! Proportions are similar.\")\n",
    "    print(\"üéâ Task 1.3 Complete!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Proportions differ - did you use stratify=y?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ü§ñ Exercise 2: Building Your First Model\n",
    "\n",
    "Now let's train a model and make predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Train a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model trained!\n",
      "‚úÖ Predictions made for 114 test samples\n",
      "\n",
      "First 10 predictions: [0 1 0 1 0 1 1 0 0 0]\n",
      "First 10 actual:      [0 1 0 1 0 1 1 0 0 0]\n",
      "\n",
      "üéâ Task 2.1 Complete!\n"
     ]
    }
   ],
   "source": [
    "# TODO 2.1: Create and train a Logistic Regression model\n",
    "# Steps:\n",
    "#   1. Create a LogisticRegression model (set max_iter=1000)\n",
    "#   2. Fit it on the training data\n",
    "#   3. Make predictions on the test set\n",
    "\n",
    "# Your code here:\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Validation (Don't modify)\n",
    "print(f\"‚úÖ Model trained!\")\n",
    "print(f\"‚úÖ Predictions made for {len(y_pred)} test samples\")\n",
    "print(f\"\\nFirst 10 predictions: {y_pred[:10]}\")\n",
    "print(f\"First 10 actual:      {y_test[:10]}\")\n",
    "print(\"\\nüéâ Task 2.1 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Calculate Basic Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "  Accuracy:  0.9649 (96.5%)\n",
      "  Precision: 0.9595 (95.9%)\n",
      "  Recall:    0.9861 (98.6%)\n",
      "  F1-Score:  0.9726 (97.3%)\n",
      "\n",
      "‚úÖ Great performance! Above 90% accuracy!\n",
      "üéâ Task 2.2 Complete!\n"
     ]
    }
   ],
   "source": [
    "# TODO 2.2: Calculate accuracy, precision, recall, and F1-score\n",
    "# Use the functions from sklearn.metrics\n",
    "\n",
    "# Your code here:\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "precision = precision_score(y_test,y_pred)\n",
    "recall = recall_score(y_test,y_pred)\n",
    "f1 = f1_score(y_test,y_pred)\n",
    "\n",
    "# Validation (Don't modify)\n",
    "print(\"Model Performance:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f} ({accuracy:.1%})\")\n",
    "print(f\"  Precision: {precision:.4f} ({precision:.1%})\")\n",
    "print(f\"  Recall:    {recall:.4f} ({recall:.1%})\")\n",
    "print(f\"  F1-Score:  {f1:.4f} ({f1:.1%})\")\n",
    "\n",
    "if accuracy > 0.9:\n",
    "    print(\"\\n‚úÖ Great performance! Above 90% accuracy!\")\n",
    "    print(\"üéâ Task 2.2 Complete!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Accuracy is {accuracy:.1%} - check your code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé≠ Exercise 3: Understanding Confusion Matrix\n",
    "\n",
    "The confusion matrix is the foundation of all classification metrics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: Create Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[39  3]\n",
      " [ 1 71]]\n",
      "\n",
      "Breakdown:\n",
      "  True Negatives (TN):  39\n",
      "  False Positives (FP): 3\n",
      "  False Negatives (FN): 1\n",
      "  True Positives (TP):  71\n",
      "\n",
      "üéâ Task 3.1 Complete!\n"
     ]
    }
   ],
   "source": [
    "# TODO 3.1: Calculate the confusion matrix\n",
    "# Then extract True Negatives, False Positives, False Negatives, True Positives\n",
    "\n",
    "# Your code here:\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Validation (Don't modify)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  True Negatives (TN):  {tn}\")\n",
    "print(f\"  False Positives (FP): {fp}\")\n",
    "print(f\"  False Negatives (FN): {fn}\")\n",
    "print(f\"  True Positives (TP):  {tp}\")\n",
    "\n",
    "assert tn + fp + fn + tp == len(y_test), \"‚ùå Values don't add up!\"\n",
    "print(\"\\nüéâ Task 3.1 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Calculate Metrics from Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Calculations:\n",
      "  Accuracy:  0.9649\n",
      "  Precision: 0.9595\n",
      "  Recall:    0.9861\n",
      "\n",
      "Compare with sklearn:\n",
      "  Accuracy:  0.9649\n",
      "  Precision: 0.9595\n",
      "  Recall:    0.9861\n",
      "\n",
      "‚úÖ Perfect match! You understand the formulas!\n",
      "üéâ Task 3.2 Complete!\n"
     ]
    }
   ],
   "source": [
    "# TODO 3.2: Calculate metrics manually from TP, TN, FP, FN\n",
    "# This helps you understand what each metric really means!\n",
    "\n",
    "# Formulas:\n",
    "# Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "# Precision = TP / (TP + FP)\n",
    "# Recall = TP / (TP + FN)\n",
    "\n",
    "# Your code here:\n",
    "manual_accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "manual_precision = tp / (tp + fp)\n",
    "manual_recall = tp / (tp + fn)\n",
    "\n",
    "# Validation (Don't modify)\n",
    "print(\"Manual Calculations:\")\n",
    "print(f\"  Accuracy:  {manual_accuracy:.4f}\")\n",
    "print(f\"  Precision: {manual_precision:.4f}\")\n",
    "print(f\"  Recall:    {manual_recall:.4f}\")\n",
    "\n",
    "print(\"\\nCompare with sklearn:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "\n",
    "if abs(manual_accuracy - accuracy) < 0.001:\n",
    "    print(\"\\n‚úÖ Perfect match! You understand the formulas!\")\n",
    "    print(\"üéâ Task 3.2 Complete!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Numbers don't match - check your formulas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìà Exercise 4: Regression Metrics\n",
    "\n",
    "Now let's practice with regression (predicting numbers)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Train a Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression dataset loaded: 442 samples\n",
      "\n",
      "‚úÖ Model trained and predictions made!\n",
      "Sample predictions: [0.62168355 0.17803853 0.27512068 1.1320503  1.11698318]\n",
      "Sample actual:      [1 0 0 1 1]\n",
      "\n",
      "üéâ Task 4.1 Complete!\n"
     ]
    }
   ],
   "source": [
    "# TODO 4.1: Load diabetes dataset, split, and train a model\n",
    "# This is a regression problem (predicting disease progression)\n",
    "\n",
    "# Load data\n",
    "diabetes = load_diabetes()\n",
    "X_reg = diabetes.data\n",
    "y_reg = diabetes.target\n",
    "\n",
    "print(f\"Regression dataset loaded: {len(X_reg)} samples\")\n",
    "\n",
    "# Your code here:\n",
    "# 1. Split into train/test (80/20, random_state=42)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y, test_size= 0.2, random_state=42)\n",
    "\n",
    "# 2. Create and train a LinearRegression model\n",
    "reg_model = LinearRegression()\n",
    "reg_model.fit(X_train_reg,y_train_reg)\n",
    "\n",
    "# 3. Make predictions on test set\n",
    "y_pred_reg = reg_model.predict(X_test_reg)\n",
    "\n",
    "# Validation (Don't modify)\n",
    "print(f\"\\n‚úÖ Model trained and predictions made!\")\n",
    "print(f\"Sample predictions: {y_pred_reg[:5]}\")\n",
    "print(f\"Sample actual:      {y_test_reg[:5]}\")\n",
    "print(\"\\nüéâ Task 4.1 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Calculate Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Metrics:\n",
      "  MAE:  0.20\n",
      "  MSE:  0.06\n",
      "  RMSE: 0.25\n",
      "  R¬≤:   0.7271\n",
      "\n",
      "üí° Interpretation:\n",
      "  On average, predictions are off by 0.2 units (MAE)\n",
      "  Model explains 72.7% of variance in disease progression\n",
      "\n",
      "‚úÖ R¬≤ > 0.3 is reasonable for this dataset!\n",
      "üéâ Task 4.2 Complete!\n"
     ]
    }
   ],
   "source": [
    "# TODO 4.2: Calculate MAE, MSE, RMSE, and R¬≤\n",
    "\n",
    "# Your code here:\n",
    "mae = mean_absolute_error(y_test_reg , y_pred_reg)\n",
    "mse = mean_squared_error(y_test_reg , y_pred_reg)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_reg , y_pred_reg)\n",
    "\n",
    "# Validation (Don't modify)\n",
    "print(\"Regression Metrics:\")\n",
    "print(f\"  MAE:  {mae:.2f}\")\n",
    "print(f\"  MSE:  {mse:.2f}\")\n",
    "print(f\"  RMSE: {rmse:.2f}\")\n",
    "print(f\"  R¬≤:   {r2:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "print(f\"  On average, predictions are off by {mae:.1f} units (MAE)\")\n",
    "print(f\"  Model explains {r2*100:.1f}% of variance in disease progression\")\n",
    "\n",
    "if r2 > 0.3:\n",
    "    print(f\"\\n‚úÖ R¬≤ > 0.3 is reasonable for this dataset!\")\n",
    "    print(\"üéâ Task 4.2 Complete!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è R¬≤ is low - check your code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Exercise 5: Choosing the Right Metric\n",
    "\n",
    "Understanding WHEN to use which metric is critical!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1: Metric Selection Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scenario 1: Correct! We don't want false positives (important email marked as spam)\n",
      "‚úÖ Scenario 2: Correct! We must catch all cancer cases (minimize false negatives)\n",
      "‚úÖ Scenario 3: Correct! RMSE penalizes large errors more than MAE\n",
      "‚úÖ Scenario 4: Correct! F1 balances precision and recall for imbalanced classes\n",
      "\n",
      "Score: 4/4\n",
      "üéâ Perfect! You understand metric selection!\n",
      "üéâ Task 5.1 Complete!\n"
     ]
    }
   ],
   "source": [
    "# TODO 5.1: For each scenario, choose the best metric\n",
    "# Options: 'accuracy', 'precision', 'recall', 'f1', 'mae', 'rmse', 'r2'\n",
    "\n",
    "# Scenario 1: Email spam detection - we DON'T want important emails in spam\n",
    "scenario_1_metric = \"precision\"  # Your answer\n",
    "\n",
    "# Scenario 2: Cancer detection - we CAN'T miss cancer cases (false negatives are deadly)\n",
    "scenario_2_metric = \"recall\"  # Your answer\n",
    "\n",
    "# Scenario 3: Predicting house prices - large errors are worse than small errors\n",
    "scenario_3_metric = \"rmse\"  # Your answer\n",
    "\n",
    "# Scenario 4: Customer churn prediction - need balance, classes imbalanced\n",
    "scenario_4_metric = \"f1\"  # Your answer\n",
    "\n",
    "# Validation (Don't modify)\n",
    "answers = {\n",
    "    1: ('precision', \"We don't want false positives (important email marked as spam)\"),\n",
    "    2: ('recall', \"We must catch all cancer cases (minimize false negatives)\"),\n",
    "    3: ('rmse', \"RMSE penalizes large errors more than MAE\"),\n",
    "    4: ('f1', \"F1 balances precision and recall for imbalanced classes\")\n",
    "}\n",
    "\n",
    "score = 0\n",
    "your_answers = [scenario_1_metric, scenario_2_metric, scenario_3_metric, scenario_4_metric]\n",
    "\n",
    "for i, (correct, explanation) in answers.items():\n",
    "    if your_answers[i-1].lower() == correct:\n",
    "        print(f\"‚úÖ Scenario {i}: Correct! {explanation}\")\n",
    "        score += 1\n",
    "    else:\n",
    "        print(f\"‚ùå Scenario {i}: Expected '{correct}'. {explanation}\")\n",
    "\n",
    "print(f\"\\nScore: {score}/4\")\n",
    "if score == 4:\n",
    "    print(\"üéâ Perfect! You understand metric selection!\")\n",
    "    print(\"üéâ Task 5.1 Complete!\")\n",
    "elif score >= 2:\n",
    "    print(\"üëç Good! Review the wrong ones.\")\n",
    "else:\n",
    "    print(\"üìö Review Notebook 02 on when to use each metric.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÜ Lab Complete!\n",
    "\n",
    "### What You Practiced:\n",
    "\n",
    "‚úÖ **Exercise 1**: Train/test split with stratification  \n",
    "‚úÖ **Exercise 2**: Training models and making predictions  \n",
    "‚úÖ **Exercise 3**: Understanding confusion matrices  \n",
    "‚úÖ **Exercise 4**: Regression metrics (MAE, RMSE, R¬≤)  \n",
    "‚úÖ **Exercise 5**: Choosing the right metric  \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Always stratify** for classification problems\n",
    "2. **Test set is sacred** - never touch during training!\n",
    "3. **Different problems need different metrics**\n",
    "4. **Confusion matrix** is the foundation of all classification metrics\n",
    "5. **R¬≤ shows explanatory power**, RMSE shows prediction error\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Move to **Lab 02** for cross-validation practice\n",
    "- Review concepts you struggled with\n",
    "- Try modifying the code to deepen understanding\n",
    "\n",
    "**Great job! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
